{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84336a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 16:34:25,819 | INFO    | Stage 13 using run folder: 20250609\n",
      "2025-06-10 16:34:25,836 | INFO    | Input artefacts:\n",
      "  · scores: stage12\\Stage12B_PCA_Scores.csv\n",
      "  · labels: stage12\\Stage12B_ClusterLabels.csv\n",
      "  · rise: stage11\\11_RISE_Probabilities_All.csv\n",
      "  · stage3: stage03\\Stage3_Data_WithRatios.csv\n",
      "c:\\Users\\Jason Pohl\\miniconda3\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\Jason Pohl\\miniconda3\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"c:\\Users\\Jason Pohl\\miniconda3\\lib\\subprocess.py\", line 493, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"c:\\Users\\Jason Pohl\\miniconda3\\lib\\subprocess.py\", line 858, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\Jason Pohl\\miniconda3\\lib\\subprocess.py\", line 1311, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "2025-06-10 16:34:32,035 | INFO    | ✓ Stage 13 complete – artefacts in outputs_rff\\event=2008\\20250609\\stage13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STAGE 13 QUICK SUMMARY (FY-2007) ===\n",
      "\n",
      "    Kruskal-Wallis (sector) : stat =  348.685   p = 7.57e-69\n",
      "    One-way ANOVA (cluster) :   F =   26.872   p = 2.66e-07\n",
      "\n",
      "    Bimodal sectors (ΔBIC > 10)\n",
      "    ---------------------------\n",
      "                sector  ΔBIC   n\n",
      "    Financial Services 110.3 286\n",
      "Communication Services  10.4  20\n",
      "\n",
      "    Top-5 sectors by median mean_rise_prob\n",
      "    -------------------------------------\n",
      "                        count  median    mean     std     min     max\n",
      "sectorname                                                           \n",
      "Financial Services        286  0.8029  0.7924  0.0453  0.6595  0.8421\n",
      "Consumer Cyclical          40  0.7833  0.7652  0.0522  0.6516  0.8383\n",
      "Industrials                67  0.7661  0.7593  0.0576  0.5771  0.8441\n",
      "Consumer Defensive         35  0.7634  0.7543  0.0690  0.5341  0.8427\n",
      "Communication Services     20  0.7536  0.7570  0.0666  0.6484  0.8481\n",
      "\n",
      "    Cluster summary\n",
      "    ---------------\n",
      "         count  median    mean     std     min     max\n",
      "cluster                                               \n",
      "0.0         66  0.7055  0.7022  0.0480  0.6006  0.7934\n",
      "1.0        875  0.7546  0.7473  0.0693  0.5341  0.8481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# =====================================================================\n",
    "#  STAGE 13 · Sector & Cluster Insights Dashboard     (pipeline v2)\n",
    "# =====================================================================\n",
    "\"\"\"\n",
    "Inputs expected in the same run folder (no hard-coded dates):\n",
    "\n",
    "stage12/Stage12B_PCA_Scores.csv\n",
    "stage12/Stage12B_ClusterLabels.csv\n",
    "stage11/  any CSV containing both “rise” and “prob” (or “probs”)\n",
    "stage03/Stage3_Data_WithRatios.csv\n",
    "\n",
    "Env-vars honoured\n",
    "-----------------\n",
    "SWAN_YEAR   (yyyy)             → which event folder to search\n",
    "RUN_DIR     (abs path)         → skip search; use this run directly\n",
    "OUTPUT_ROOT (default outputs_rff)\n",
    "ID_COL      (default Symbol)\n",
    "DATE_COL    (default ReportDate)\n",
    "SECTOR_COL  (default SectorName)\n",
    "SCORES_CSV, LABELS_CSV, RISE_CSV, STAGE3_CSV → override any input\n",
    "PLOT=1      → save PNG plots\n",
    "\n",
    "Outputs (<run>/stage13/)\n",
    "------------------------\n",
    "Sector_Kruskal.csv\n",
    "Cluster_ANOVA.csv\n",
    "BimodalSectors.csv\n",
    "SectorSummary.csv\n",
    "ClusterSummary.csv\n",
    "QuickSummary.txt\n",
    "(optional PNGs if PLOT=1)\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os, logging, warnings, textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import kruskal, f_oneway\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)-7s | %(message)s\")\n",
    "log = logging.getLogger(\"stage13\")\n",
    "\n",
    "# ╔══════════════════════════════════════════════════════════════════╗\n",
    "# 0 · BASIC CONFIG                                                   #\n",
    "# ╚══════════════════════════════════════════════════════════════════╝\n",
    "SWAN_YEAR   = int(os.getenv(\"SWAN_YEAR\", 2008))\n",
    "PRE_YEAR    = SWAN_YEAR - 1\n",
    "OUTPUT_ROOT = Path(os.getenv(\"OUTPUT_ROOT\", \"outputs_rff\")).expanduser()\n",
    "\n",
    "ID_COL     = os.getenv(\"ID_COL\",     \"Symbol\").lower()\n",
    "DATE_COL   = os.getenv(\"DATE_COL\",   \"ReportDate\").lower()\n",
    "SECTOR_COL = os.getenv(\"SECTOR_COL\", \"SectorName\").lower()\n",
    "\n",
    "# ╔══════════════════════════════════════════════════════════════════╗\n",
    "# 1 · LOCATE THE RUN DIRECTORY                                       #\n",
    "# ╚══════════════════════════════════════════════════════════════════╝\n",
    "if os.getenv(\"RUN_DIR\"):\n",
    "    RUN_DIR = Path(os.getenv(\"RUN_DIR\")).expanduser()\n",
    "else:\n",
    "    event_dir = OUTPUT_ROOT / f\"event={SWAN_YEAR}\"\n",
    "    if not event_dir.is_dir():\n",
    "        raise RuntimeError(f\"No directory {event_dir}\")\n",
    "    # pick most-recent run that already has Stage 12 PCA scores\n",
    "    candidates = sorted(\n",
    "        event_dir.glob(\"*/stage12/Stage12B_PCA_Scores.csv\"),\n",
    "        key=lambda p: p.stat().st_mtime,\n",
    "        reverse=True,\n",
    "    )\n",
    "    if not candidates:\n",
    "        raise RuntimeError(\"Run Stage 12 first – no PCA scores found.\")\n",
    "    RUN_DIR = candidates[0].parents[1]\n",
    "\n",
    "STAGE_DIR = RUN_DIR / \"stage13\"\n",
    "STAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "log.info(\"Stage 13 using run folder: %s\", RUN_DIR.name)\n",
    "\n",
    "# ╔══════════════════════════════════════════════════════════════════╗\n",
    "# 2 · RESOLVE INPUT FILES                                            #\n",
    "# ╚══════════════════════════════════════════════════════════════════╝\n",
    "def _resolve(env_var: str, canonical: Path, fallback_kw: list[str] | None = None) -> Path:\n",
    "    \"\"\"env override → canonical path → first file matching all keywords.\"\"\"\n",
    "    if os.getenv(env_var):\n",
    "        p = Path(os.getenv(env_var)).expanduser()\n",
    "        if not p.is_file():\n",
    "            raise FileNotFoundError(f\"{env_var}='{p}' not found\")\n",
    "        return p\n",
    "    if canonical.is_file():\n",
    "        return canonical\n",
    "    if fallback_kw:\n",
    "        kws = [k.lower() for k in fallback_kw]\n",
    "        for fp in canonical.parent.glob(\"*.csv\"):\n",
    "            nm = fp.name.lower()\n",
    "            if all(k in nm for k in kws):\n",
    "                return fp\n",
    "    raise FileNotFoundError(f\"{canonical.name} (or {env_var}) not found\")\n",
    "\n",
    "paths = {\n",
    "    \"scores\" : _resolve(\"SCORES_CSV\",\n",
    "                        RUN_DIR / \"stage12\" / \"Stage12B_PCA_Scores.csv\"),\n",
    "    \"labels\" : _resolve(\"LABELS_CSV\",\n",
    "                        RUN_DIR / \"stage12\" / \"Stage12B_ClusterLabels.csv\"),\n",
    "    \"rise\"   : _resolve(\"RISE_CSV\",\n",
    "                        RUN_DIR / \"stage11\" / \"Stage11_RISE_Probabilities_All.csv\",\n",
    "                        fallback_kw=[\"rise\", \"prob\"]),\n",
    "    \"stage3\" : _resolve(\"STAGE3_CSV\",\n",
    "                        RUN_DIR / \"stage03\" / \"Stage3_Data_WithRatios.csv\"),\n",
    "}\n",
    "log.info(\"Input artefacts:\\n%s\",\n",
    "         \"\\n\".join(f\"  · {k}: {v.relative_to(RUN_DIR)}\" for k,v in paths.items()))\n",
    "\n",
    "# ╔══════════════════════════════════════════════════════════════════╗\n",
    "# 3 · LOAD DATA                                                      #\n",
    "# ╚══════════════════════════════════════════════════════════════════╝\n",
    "def _ld(fp: Path, parse_dates: bool = False) -> pd.DataFrame:\n",
    "    df = pd.read_csv(fp, low_memory=False)\n",
    "    df.columns = df.columns.str.lower().str.strip()\n",
    "    if parse_dates and DATE_COL in df.columns:\n",
    "        df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "scores  = _ld(paths[\"scores\"])\n",
    "labels  = _ld(paths[\"labels\"])\n",
    "rise    = _ld(paths[\"rise\"],   parse_dates=True)\n",
    "stage3  = _ld(paths[\"stage3\"], parse_dates=True)\n",
    "\n",
    "snap = stage3[stage3[DATE_COL].dt.year == PRE_YEAR].copy()\n",
    "if snap.empty:\n",
    "    raise RuntimeError(f\"No FY-{PRE_YEAR} rows in Stage 3 snapshot\")\n",
    "\n",
    "prob_cols = [c for c in rise.columns if \"prob\" in c]\n",
    "if not prob_cols:\n",
    "    raise RuntimeError(\"No probability columns in Stage 11 file\")\n",
    "\n",
    "df = (snap[[ID_COL, DATE_COL, SECTOR_COL]]\n",
    "        .merge(scores, on=ID_COL, how=\"left\")\n",
    "        .merge(labels, on=ID_COL, how=\"left\")\n",
    "        .merge(rise[[ID_COL, DATE_COL] + prob_cols],\n",
    "               on=[ID_COL, DATE_COL], how=\"left\"))\n",
    "\n",
    "df[\"mean_rise_prob\"] = (df[prob_cols]\n",
    "                          .replace([np.inf, -np.inf], np.nan)\n",
    "                          .mean(axis=1, skipna=True))\n",
    "\n",
    "# ╔══════════════════════════════════════════════════════════════════╗\n",
    "# 4 · STATISTICAL TESTS & SUMMARIES                                  #\n",
    "# ╚══════════════════════════════════════════════════════════════════╝\n",
    "sector_groups  = [g[\"mean_rise_prob\"].dropna().values\n",
    "                  for _, g in df.groupby(SECTOR_COL) if len(g) >= 5]\n",
    "kw_s, kw_p = kruskal(*sector_groups) if len(sector_groups) >= 2 else (np.nan, np.nan)\n",
    "\n",
    "cluster_groups = [g[\"mean_rise_prob\"].dropna().values\n",
    "                  for _, g in df.groupby(\"cluster\") if len(g) >= 5]\n",
    "anova_f, anova_p = f_oneway(*cluster_groups) if len(cluster_groups) >= 2 else (np.nan, np.nan)\n",
    "\n",
    "pd.DataFrame({\"KW_stat\":[kw_s], \"p_value\":[kw_p]})\\\n",
    "    .to_csv(STAGE_DIR / \"Sector_Kruskal.csv\", index=False)\n",
    "pd.DataFrame({\"ANOVA_F\":[anova_f], \"p_value\":[anova_p]})\\\n",
    "    .to_csv(STAGE_DIR / \"Cluster_ANOVA.csv\", index=False)\n",
    "\n",
    "# bimodality scan\n",
    "rows=[]\n",
    "for sec,g in df.groupby(SECTOR_COL):\n",
    "    x=g[\"mean_rise_prob\"].dropna().values.reshape(-1,1)\n",
    "    if len(x)<20: continue\n",
    "    bic1=GaussianMixture(1,random_state=0).fit(x).bic(x)\n",
    "    bic2=GaussianMixture(2,random_state=0).fit(x).bic(x)\n",
    "    if bic1 - bic2 > 10:\n",
    "        rows.append({\"sector\":sec,\"ΔBIC\":round(bic1-bic2,1),\"n\":len(x)})\n",
    "bimodal = pd.DataFrame(rows).sort_values(\"ΔBIC\", ascending=False)\n",
    "bimodal.to_csv(STAGE_DIR / \"BimodalSectors.csv\", index=False)\n",
    "\n",
    "sector_sum  = (df.groupby(SECTOR_COL)[\"mean_rise_prob\"]\n",
    "                 .agg(['count','median','mean','std','min','max'])\n",
    "                 .round(4).sort_values(\"median\", ascending=False))\n",
    "cluster_sum = (df.groupby(\"cluster\")[\"mean_rise_prob\"]\n",
    "                 .agg(['count','median','mean','std','min','max'])\n",
    "                 .round(4))\n",
    "sector_sum .to_csv(STAGE_DIR / \"SectorSummary.csv\")\n",
    "cluster_sum.to_csv(STAGE_DIR / \"ClusterSummary.csv\")\n",
    "\n",
    "# quick summary\n",
    "quick = textwrap.dedent(f\"\"\"\n",
    "    === STAGE 13 QUICK SUMMARY (FY-{PRE_YEAR}) ===\n",
    "\n",
    "    Kruskal-Wallis (sector) : stat = {kw_s:8.3f}   p = {kw_p:.3g}\n",
    "    One-way ANOVA (cluster) :   F = {anova_f:8.3f}   p = {anova_p:.3g}\n",
    "\n",
    "    Bimodal sectors (ΔBIC > 10)\n",
    "    ---------------------------\n",
    "{bimodal.to_string(index=False) if not bimodal.empty else '      (none detected)'}\n",
    "\n",
    "    Top-5 sectors by median mean_rise_prob\n",
    "    -------------------------------------\n",
    "{sector_sum.head(5).to_string()}\n",
    "\n",
    "    Cluster summary\n",
    "    ---------------\n",
    "{cluster_sum.to_string()}\n",
    "\"\"\").strip()\n",
    "(STAGE_DIR / \"QuickSummary.txt\").write_text(quick, encoding=\"utf-8\")\n",
    "print(\"\\n\"+quick+\"\\n\")\n",
    "\n",
    "# ╔══════════════════════════════════════════════════════════════════╗\n",
    "# 5 · OPTIONAL PLOTS                                                 #\n",
    "# ╚══════════════════════════════════════════════════════════════════╝\n",
    "if os.getenv(\"PLOT\", \"0\") == \"1\":\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    pc_cols = [c for c in df.columns if c.startswith(\"pc\")]\n",
    "\n",
    "    if len(pc_cols) >= 2:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.scatterplot(x=pc_cols[0], y=pc_cols[1],\n",
    "                        hue=SECTOR_COL, data=df,\n",
    "                        palette=\"tab20\", s=40, linewidth=.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(STAGE_DIR / \"PC1_PC2_by_sector.png\", dpi=110)\n",
    "\n",
    "    top_secs = df[SECTOR_COL].value_counts().head(12).index\n",
    "    plt.figure(figsize=(9,4))\n",
    "    sns.boxplot(x=SECTOR_COL, y=\"mean_rise_prob\",\n",
    "                data=df[df[SECTOR_COL].isin(top_secs)])\n",
    "    plt.xticks(rotation=35, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(STAGE_DIR / \"RiseProb_by_sector.png\", dpi=110)\n",
    "\n",
    "    plt.figure(figsize=(5,3))\n",
    "    sns.boxplot(x=\"cluster\", y=\"mean_rise_prob\", data=df)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(STAGE_DIR / \"RiseProb_by_cluster.png\", dpi=110)\n",
    "\n",
    "log.info(\"✓ Stage 13 complete – artefacts in %s\", STAGE_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
