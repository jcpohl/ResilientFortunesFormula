{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84336a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 16:34:25,819 | INFO    | Stage 13 using run folder: 20250609\n",
      "2025-06-10 16:34:25,836 | INFO    | Input artefacts:\n",
      "  · scores: stage12\\Stage12B_PCA_Scores.csv\n",
      "  · labels: stage12\\Stage12B_ClusterLabels.csv\n",
      "  · rise: stage11\\11_RISE_Probabilities_All.csv\n",
      "  · stage3: stage03\\Stage3_Data_WithRatios.csv\n",
      "c:\\Users\\Jason Pohl\\miniconda3\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\Jason Pohl\\miniconda3\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"c:\\Users\\Jason Pohl\\miniconda3\\lib\\subprocess.py\", line 493, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"c:\\Users\\Jason Pohl\\miniconda3\\lib\\subprocess.py\", line 858, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\Jason Pohl\\miniconda3\\lib\\subprocess.py\", line 1311, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "2025-06-10 16:34:32,035 | INFO    | ✓ Stage 13 complete – artefacts in outputs_rff\\event=2008\\20250609\\stage13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STAGE 13 QUICK SUMMARY (FY-2007) ===\n",
      "\n",
      "    Kruskal-Wallis (sector) : stat =  348.685   p = 7.57e-69\n",
      "    One-way ANOVA (cluster) :   F =   26.872   p = 2.66e-07\n",
      "\n",
      "    Bimodal sectors (ΔBIC > 10)\n",
      "    ---------------------------\n",
      "                sector  ΔBIC   n\n",
      "    Financial Services 110.3 286\n",
      "Communication Services  10.4  20\n",
      "\n",
      "    Top-5 sectors by median mean_rise_prob\n",
      "    -------------------------------------\n",
      "                        count  median    mean     std     min     max\n",
      "sectorname                                                           \n",
      "Financial Services        286  0.8029  0.7924  0.0453  0.6595  0.8421\n",
      "Consumer Cyclical          40  0.7833  0.7652  0.0522  0.6516  0.8383\n",
      "Industrials                67  0.7661  0.7593  0.0576  0.5771  0.8441\n",
      "Consumer Defensive         35  0.7634  0.7543  0.0690  0.5341  0.8427\n",
      "Communication Services     20  0.7536  0.7570  0.0666  0.6484  0.8481\n",
      "\n",
      "    Cluster summary\n",
      "    ---------------\n",
      "         count  median    mean     std     min     max\n",
      "cluster                                               \n",
      "0.0         66  0.7055  0.7022  0.0480  0.6006  0.7934\n",
      "1.0        875  0.7546  0.7473  0.0693  0.5341  0.8481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# =====================================================================\n",
    "#  STAGE 13 · Sector & Cluster Insights Dashboard        (pipeline v2)\n",
    "# =====================================================================\n",
    "\"\"\"\n",
    "Inputs\n",
    "------\n",
    "<run>/stage12/Stage12B_PCA_Scores.csv\n",
    "<run>/stage12/Stage12B_ClusterLabels.csv\n",
    "<run>/stage11/11_RISE_Probabilities_All.csv  (or any CSV containing both\n",
    "                                             “rise” and “prob” in the name)\n",
    "<run>/stage03/Stage3_Data_WithRatios.csv\n",
    "\n",
    "Outputs  <run>/stage13/\n",
    "  Sector_Kruskal.csv\n",
    "  Cluster_ANOVA.csv\n",
    "  BimodalSectors.csv\n",
    "  SectorSummary.csv\n",
    "  ClusterSummary.csv\n",
    "  QuickSummary.txt\n",
    "  (plus PNG plots if PLOT=1)\n",
    "\n",
    "Env-vars honoured\n",
    "  SWAN_YEAR  · RUN_DIR · OUTPUT_ROOT · ID_COL · DATE_COL · SECTOR_COL\n",
    "  SCORES_CSV · LABELS_CSV · RISE_CSV · STAGE3_CSV · PLOT\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "# ── stdlib / 3-rd-party ────────────────────────────────────────────\n",
    "from pathlib import Path\n",
    "import os, logging, warnings, textwrap\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "from scipy.stats import kruskal, f_oneway\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# ── shared helpers ─────────────────────────────────────────────────\n",
    "from pipeline_utils import load_cfg, resolve_run_dir          # NEW\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "plt.rcParams[\"figure.dpi\"] = 110\n",
    "\n",
    "# ╔══════════════════════════════════════════════════════════════════╗\n",
    "# 0 · BOOTSTRAP  (config · run-folder · logger)                       #\n",
    "# ╚══════════════════════════════════════════════════════════════════╝\n",
    "CFG: Dict      = load_cfg()\n",
    "EVENTS: Dict   = {str(k): v for k, v in CFG.get(\"events\", {}).items()}\n",
    "\n",
    "SWAN_YEAR_STR  = os.getenv(\"SWAN_YEAR\") or next(iter(EVENTS))\n",
    "if SWAN_YEAR_STR not in EVENTS:\n",
    "    raise KeyError(f\"SWAN_YEAR={SWAN_YEAR_STR} not listed in events block\")\n",
    "SWAN_YEAR      = int(SWAN_YEAR_STR)\n",
    "PRE_YEAR       = SWAN_YEAR - 1\n",
    "\n",
    "RUN_DIR        = resolve_run_dir(must_have=\"stage12/Stage12B_PCA_Scores.csv\")\n",
    "RUN_DATE       = RUN_DIR.name\n",
    "\n",
    "STAGE_DIR      = RUN_DIR / \"stage13\"\n",
    "STAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)-7s | %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(STAGE_DIR / \"stage13.log\", mode=\"w\", encoding=\"utf-8\"),\n",
    "        logging.StreamHandler(),\n",
    "    ],\n",
    ")\n",
    "log = logging.getLogger(\"stage13\")\n",
    "log.info(\"==========  STAGE 13 ==========\")\n",
    "log.info(\"RUN_DIR=%s  SWAN_YEAR=%s  RUN_DATE=%s\", RUN_DIR, SWAN_YEAR, RUN_DATE)\n",
    "\n",
    "# ╔══════════════════════════════════════════════════════════════════╗\n",
    "# 1 · ENV ALIASES & COLUMN NAMES                                     #\n",
    "# ╚══════════════════════════════════════════════════════════════════╝\n",
    "OUTPUT_ROOT = Path(os.getenv(\"OUTPUT_ROOT\", \"outputs_rff\")).expanduser()\n",
    "\n",
    "ID_COL     = os.getenv(\"ID_COL\",     \"Symbol\").lower()\n",
    "DATE_COL   = os.getenv(\"DATE_COL\",   \"ReportDate\").lower()\n",
    "SECTOR_COL = os.getenv(\"SECTOR_COL\", \"SectorName\").lower()\n",
    "\n",
    "# ╔══════════════════════════════════════════════════════════════════╗\n",
    "# 2 · RESOLVE INPUT FILES                                            #\n",
    "# ╚══════════════════════════════════════════════════════════════════╝\n",
    "def _resolve(env_var: str, canonical: Path,\n",
    "             fallback_kw: List[str] | None = None) -> Path:\n",
    "    \"\"\"env override → canonical → first *.csv* matching *fallback_kw*.\"\"\"\n",
    "    if os.getenv(env_var):\n",
    "        p = Path(os.getenv(env_var)).expanduser()\n",
    "        if not p.is_file():\n",
    "            raise FileNotFoundError(f\"{env_var}={p} not found\")\n",
    "        return p\n",
    "    if canonical.is_file():\n",
    "        return canonical\n",
    "    if fallback_kw:\n",
    "        want = [k.lower() for k in fallback_kw]\n",
    "        for fp in canonical.parent.glob(\"*.csv\"):\n",
    "            if all(k in fp.name.lower() for k in want):\n",
    "                return fp\n",
    "    raise FileNotFoundError(f\"{canonical.name} (or {env_var}) not found\")\n",
    "\n",
    "paths = {\n",
    "    \"scores\": _resolve(\"SCORES_CSV\",\n",
    "                       RUN_DIR / \"stage12\" / \"Stage12B_PCA_Scores.csv\"),\n",
    "    \"labels\": _resolve(\"LABELS_CSV\",\n",
    "                       RUN_DIR / \"stage12\" / \"Stage12B_ClusterLabels.csv\"),\n",
    "    \"rise\"  : _resolve(\"RISE_CSV\",\n",
    "                       RUN_DIR / \"stage11\" / \"11_RISE_Probabilities_All.csv\",\n",
    "                       fallback_kw=[\"rise\", \"prob\"]),\n",
    "    \"stage3\": _resolve(\"STAGE3_CSV\",\n",
    "                       RUN_DIR / \"stage03\" / \"Stage3_Data_WithRatios.csv\"),\n",
    "}\n",
    "log.info(\"Input artefacts resolved:\")\n",
    "for k, v in paths.items():\n",
    "    log.info(\"  · %-7s %s\", k, v.relative_to(RUN_DIR))\n",
    "\n",
    "# ╔══════════════════════════════════════════════════════════════════╗\n",
    "# 3 · LOAD DATA                                                      #\n",
    "# ╚══════════════════════════════════════════════════════════════════╝\n",
    "def _ld(fp: Path, parse_dates: bool = False) -> pd.DataFrame:\n",
    "    df = pd.read_csv(fp, low_memory=False)\n",
    "    df.columns = df.columns.str.lower().str.strip()\n",
    "    if parse_dates and DATE_COL in df.columns:\n",
    "        df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "scores  = _ld(paths[\"scores\"])\n",
    "labels  = _ld(paths[\"labels\"])\n",
    "rise    = _ld(paths[\"rise\"],   parse_dates=True)\n",
    "stage3  = _ld(paths[\"stage3\"], parse_dates=True)\n",
    "\n",
    "snap = stage3[stage3[DATE_COL].dt.year == PRE_YEAR].copy()\n",
    "if snap.empty:\n",
    "    raise RuntimeError(f\"No FY-{PRE_YEAR} rows in Stage-03 snapshot\")\n",
    "\n",
    "prob_cols = [c for c in rise.columns if \"prob\" in c]\n",
    "if not prob_cols:\n",
    "    raise RuntimeError(\"No probability columns in Stage-11 file\")\n",
    "\n",
    "df = (snap[[ID_COL, DATE_COL, SECTOR_COL]]\n",
    "        .merge(scores, on=ID_COL, how=\"left\")\n",
    "        .merge(labels, on=ID_COL, how=\"left\")\n",
    "        .merge(rise[[ID_COL, DATE_COL] + prob_cols],\n",
    "               on=[ID_COL, DATE_COL], how=\"left\"))\n",
    "df[\"mean_rise_prob\"] = df[prob_cols].replace([np.inf, -np.inf], np.nan) \\\n",
    "                                    .mean(axis=1, skipna=True)\n",
    "\n",
    "# ╔══════════════════════════════════════════════════════════════════╗\n",
    "# 4 · STATISTICAL TESTS & SUMMARIES  (UNCHANGED)                     #\n",
    "# ╚══════════════════════════════════════════════════════════════════╝\n",
    "sector_groups  = [g[\"mean_rise_prob\"].dropna().values\n",
    "                  for _, g in df.groupby(SECTOR_COL) if len(g) >= 5]\n",
    "kw_s, kw_p = kruskal(*sector_groups) if len(sector_groups) >= 2 else (np.nan, np.nan)\n",
    "\n",
    "cluster_groups = [g[\"mean_rise_prob\"].dropna().values\n",
    "                  for _, g in df.groupby(\"cluster\") if len(g) >= 5]\n",
    "anova_f, anova_p = f_oneway(*cluster_groups) if len(cluster_groups) >= 2 else (np.nan, np.nan)\n",
    "\n",
    "pd.DataFrame({\"KW_stat\": [kw_s], \"p_value\": [kw_p]})\\\n",
    "    .to_csv(STAGE_DIR / \"Sector_Kruskal.csv\", index=False)\n",
    "pd.DataFrame({\"ANOVA_F\": [anova_f], \"p_value\": [anova_p]})\\\n",
    "    .to_csv(STAGE_DIR / \"Cluster_ANOVA.csv\", index=False)\n",
    "\n",
    "rows = []\n",
    "for sec, g in df.groupby(SECTOR_COL):\n",
    "    x = g[\"mean_rise_prob\"].dropna().values.reshape(-1, 1)\n",
    "    if len(x) < 20:\n",
    "        continue\n",
    "    bic1 = GaussianMixture(1, random_state=0).fit(x).bic(x)\n",
    "    bic2 = GaussianMixture(2, random_state=0).fit(x).bic(x)\n",
    "    if bic1 - bic2 > 10:\n",
    "        rows.append({\"sector\": sec, \"ΔBIC\": round(bic1 - bic2, 1), \"n\": len(x)})\n",
    "bimodal = pd.DataFrame(rows).sort_values(\"ΔBIC\", ascending=False)\n",
    "bimodal.to_csv(STAGE_DIR / \"BimodalSectors.csv\", index=False)\n",
    "\n",
    "sector_sum  = (df.groupby(SECTOR_COL)[\"mean_rise_prob\"]\n",
    "                 .agg(['count', 'median', 'mean', 'std', 'min', 'max'])\n",
    "                 .round(4).sort_values(\"median\", ascending=False))\n",
    "cluster_sum = (df.groupby(\"cluster\")[\"mean_rise_prob\"]\n",
    "                 .agg(['count', 'median', 'mean', 'std', 'min', 'max'])\n",
    "                 .round(4))\n",
    "sector_sum.to_csv(STAGE_DIR / \"SectorSummary.csv\")\n",
    "cluster_sum.to_csv(STAGE_DIR / \"ClusterSummary.csv\")\n",
    "\n",
    "quick = textwrap.dedent(f\"\"\"\n",
    "    === STAGE 13 QUICK SUMMARY (FY-{PRE_YEAR}) ===\n",
    "\n",
    "    Kruskal-Wallis (sector) : stat = {kw_s:8.3f}   p = {kw_p:.3g}\n",
    "    One-way ANOVA (cluster) :   F = {anova_f:8.3f}   p = {anova_p:.3g}\n",
    "\n",
    "    Bimodal sectors (ΔBIC > 10)\n",
    "    ---------------------------\n",
    "{bimodal.to_string(index=False) if not bimodal.empty else '      (none detected)'}\n",
    "\n",
    "    Top-5 sectors by median mean_rise_prob\n",
    "    -------------------------------------\n",
    "{sector_sum.head(5).to_string()}\n",
    "\n",
    "    Cluster summary\n",
    "    ---------------\n",
    "{cluster_sum.to_string()}\n",
    "\"\"\").strip()\n",
    "(STAGE_DIR / \"QuickSummary.txt\").write_text(quick, encoding=\"utf-8\")\n",
    "print(\"\\n\" + quick + \"\\n\")\n",
    "\n",
    "# ╔══════════════════════════════════════════════════════════════════╗\n",
    "# 5 · OPTIONAL PLOTS                                                 #\n",
    "# ╚══════════════════════════════════════════════════════════════════╝\n",
    "if os.getenv(\"PLOT\", \"0\") == \"1\":\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    pc_cols = [c for c in df.columns if c.startswith(\"pc\")]\n",
    "\n",
    "    if len(pc_cols) >= 2:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.scatterplot(x=pc_cols[0], y=pc_cols[1],\n",
    "                        hue=SECTOR_COL, data=df,\n",
    "                        palette=\"tab20\", s=40, linewidth=.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(STAGE_DIR / \"PC1_PC2_by_sector.png\", dpi=110)\n",
    "\n",
    "    top_secs = df[SECTOR_COL].value_counts().head(12).index\n",
    "    plt.figure(figsize=(9, 4))\n",
    "    sns.boxplot(x=SECTOR_COL, y=\"mean_rise_prob\",\n",
    "                data=df[df[SECTOR_COL].isin(top_secs)])\n",
    "    plt.xticks(rotation=35, ha=\"right\"); plt.tight_layout()\n",
    "    plt.savefig(STAGE_DIR / \"RiseProb_by_sector.png\", dpi=110)\n",
    "\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    sns.boxplot(x=\"cluster\", y=\"mean_rise_prob\", data=df)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(STAGE_DIR / \"RiseProb_by_cluster.png\", dpi=110)\n",
    "\n",
    "log.info(\"✅ Stage 13 complete – artefacts in %s\", STAGE_DIR)\n",
    "print(f\"\\n✅ Stage 13 complete – outputs in {STAGE_DIR}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
