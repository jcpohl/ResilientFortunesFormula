{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af24517a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage01] SWAN_YEAR not set – defaulting to 2008\n",
      "2025-06-10 12:44:45,722 | INFO    | ==========  STAGE 01: DATA LOAD & PRE-PROCESSING ==========\n",
      "2025-06-10 12:44:45,723 | INFO    | Config file    : c:\\Users\\Jason Pohl\\OneDrive - Bond University\\PhD\\rff\\pipeline_config.yaml\n",
      "2025-06-10 12:44:45,724 | INFO    | INPUT_CSV      : C:\\Users\\Jason Pohl\\OneDrive - Bond University\\PhD\\rff\\NEW_DATA.csv\n",
      "2025-06-10 12:44:45,724 | INFO    | OUTPUT_DIR     : C:\\Users\\Jason Pohl\\OneDrive - Bond University\\PhD\\rff\\outputs_rff\\event=2008\\20250610\\stage01\n",
      "2025-06-10 12:44:45,725 | INFO    | SWAN_YEAR      : 2008  |  RUN_DATE: 20250610\n",
      "2025-06-10 12:44:45,726 | INFO    | DATE_COL / ID_COL   = ReportDate / Symbol\n",
      "2025-06-10 12:44:45,728 | INFO    | FILTERS             = {'pct_non_na': 95, 'pct_zero': 98, 'min_unique': 10}\n",
      "2025-06-10 12:44:48,025 | INFO    | Rows loaded: 55,800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jason Pohl\\AppData\\Local\\Temp\\ipykernel_36448\\2775383609.py:100: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S.%f format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  pd.to_datetime(df[DATE_COL], errors=\"coerce\", dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-10 12:44:48,771 | INFO    | After ID/date filter: 55800 rows (100.0%)\n",
      "2025-06-10 12:44:49,325 | INFO    | Dropped 94 low-quality numeric columns\n",
      "2025-06-10 12:44:52,954 | INFO    | Final DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34862 entries, 0 to 34861\n",
      "Columns: 164 entries, Symbol to ReportDate\n",
      "dtypes: datetime64[ns](1), float64(148), int32(1), int64(4), object(10)\n",
      "memory usage: 43.5+ MB\n",
      "\n",
      "2025-06-10 12:44:52,955 | INFO    | Saved cleaned CSV → C:\\Users\\Jason Pohl\\OneDrive - Bond University\\PhD\\rff\\outputs_rff\\event=2008\\20250610\\stage01\\stage01_cleaned.csv\n",
      "2025-06-10 12:44:52,968 | INFO    | ✅ STAGE 01 complete — `data_stage_1` ready\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STAGE 01 · DATA LOAD & PRE-PROCESSING\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "• Reads run parameters from pipeline_config.yaml\n",
    "• If SWAN_YEAR is not supplied via env-var, defaults to the first event\n",
    "  key in the YAML, so the script can run with *zero* manual input.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os, logging, io, re, yaml, sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 0 · CONFIG  (locate pipeline_config.yaml in every context)\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "PIPELINE_CFG_ENV = os.getenv(\"PIPELINE_CFG\")\n",
    "\n",
    "if PIPELINE_CFG_ENV:\n",
    "    CFG_FILE = Path(PIPELINE_CFG_ENV).expanduser()\n",
    "else:\n",
    "    try:                                # normal script path\n",
    "        CFG_DIR = Path(__file__).resolve().parent\n",
    "    except NameError:                   # interactive / notebook\n",
    "        CFG_DIR = Path.cwd()\n",
    "    CFG_FILE = CFG_DIR / \"pipeline_config.yaml\"\n",
    "\n",
    "if not CFG_FILE.is_file():\n",
    "    raise FileNotFoundError(f\"pipeline_config.yaml not found at {CFG_FILE}\")\n",
    "\n",
    "with CFG_FILE.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "    CFG: dict = yaml.safe_load(fh) or {}\n",
    "\n",
    "DEFAULTS = CFG.get(\"defaults\", {})\n",
    "EVENTS   = CFG.get(\"events\",   {})\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 1 · RUN-SCOPE VARIABLES\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "SWAN_YEAR = os.getenv(\"SWAN_YEAR\")\n",
    "if SWAN_YEAR is None:\n",
    "    SWAN_YEAR = next(iter(EVENTS.keys()))   # first event in the YAML\n",
    "    print(f\"[Stage01] SWAN_YEAR not set – defaulting to {SWAN_YEAR}\")\n",
    "\n",
    "if SWAN_YEAR not in EVENTS:\n",
    "    raise KeyError(\n",
    "        f\"SWAN_YEAR={SWAN_YEAR} not present in `events:` section of {CFG_FILE}\"\n",
    "    )\n",
    "\n",
    "RUN_DATE = os.getenv(\"RUN_DATE\", datetime.utcnow().strftime(\"%Y%m%d\"))\n",
    "\n",
    "INPUT_CSV   = Path(DEFAULTS[\"INPUT_CSV\"]).expanduser()\n",
    "OUTPUT_ROOT = Path(DEFAULTS[\"OUTPUT_ROOT\"]).expanduser()\n",
    "\n",
    "DATE_COL = DEFAULTS.get(\"DATE_COL\", \"ReportDate\")\n",
    "ID_COL   = DEFAULTS.get(\"ID_COL\",   \"Symbol\")\n",
    "\n",
    "FILTERS = {\n",
    "    \"pct_non_na\": DEFAULTS.get(\"PCT_NON_NA\", 95),\n",
    "    \"pct_zero\":   DEFAULTS.get(\"PCT_ZERO\",   98),\n",
    "    \"min_unique\": DEFAULTS.get(\"MIN_UNIQUE\", 10),\n",
    "}\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 2 · OUTPUT DIR & LOGGER\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "OUTPUT_DIR = OUTPUT_ROOT / f\"event={SWAN_YEAR}\" / RUN_DATE / \"stage01\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)-7s | %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(OUTPUT_DIR / \"stage01.log\", mode=\"w\", encoding=\"utf-8\"),\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ],\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"==========  STAGE 01: DATA LOAD & PRE-PROCESSING ==========\")\n",
    "logger.info(\"Config file    : %s\", CFG_FILE)\n",
    "logger.info(\"INPUT_CSV      : %s\", INPUT_CSV)\n",
    "logger.info(\"OUTPUT_DIR     : %s\", OUTPUT_DIR)\n",
    "logger.info(\"SWAN_YEAR      : %s  |  RUN_DATE: %s\", SWAN_YEAR, RUN_DATE)\n",
    "logger.info(\"DATE_COL / ID_COL   = %s / %s\", DATE_COL, ID_COL)\n",
    "logger.info(\"FILTERS             = %s\", FILTERS)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 3 · LOAD RAW CSV\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "df = pd.read_csv(INPUT_CSV, low_memory=False)\n",
    "logger.info(\"Rows loaded: %s\", f\"{len(df):,}\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 4 · DATE PARSE & ID NORMALISATION\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "df[DATE_COL] = (\n",
    "    pd.to_datetime(df[DATE_COL], errors=\"coerce\", dayfirst=True)\n",
    "      .fillna(pd.to_datetime(df[DATE_COL], errors=\"coerce\", dayfirst=False))\n",
    ")\n",
    "bad_dates = df[df[DATE_COL].isna()]\n",
    "if not bad_dates.empty:\n",
    "    bad_dates.to_csv(OUTPUT_DIR / \"bad_dates.csv\", index=False)\n",
    "    logger.warning(\"Bad dates → %d rows saved to bad_dates.csv\", len(bad_dates))\n",
    "\n",
    "df[ID_COL] = df[ID_COL].astype(str).str.strip().str.upper()\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 5 · SMART NUMERIC COERCION\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "_num_rx = re.compile(r\"[$€£,%]\")\n",
    "\n",
    "def smart_to_num(series: pd.Series) -> pd.Series:\n",
    "    if series.dtype != \"object\":\n",
    "        return series\n",
    "    out = pd.to_numeric(series.str.replace(_num_rx, \"\", regex=True), errors=\"coerce\")\n",
    "    return out if out.notna().mean() >= 0.50 else series\n",
    "\n",
    "df = df.apply(smart_to_num)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 6 · BASIC FILTERS\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "before = len(df)\n",
    "df = df.dropna(subset=[ID_COL, DATE_COL])\n",
    "logger.info(\"After ID/date filter: %d rows (%.1f%%)\", len(df), len(df) / before * 100)\n",
    "\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "meta = pd.DataFrame({\n",
    "    \"pct_non_na\": df[num_cols].notna().mean() * 100,\n",
    "    \"pct_zero\":   (df[num_cols] == 0).mean() * 100,\n",
    "    \"n_unique\":   df[num_cols].nunique(dropna=True),\n",
    "})\n",
    "good_mask = (\n",
    "    (meta[\"pct_non_na\"] >= FILTERS[\"pct_non_na\"]) &\n",
    "    (meta[\"pct_zero\"]   <  FILTERS[\"pct_zero\"])   &\n",
    "    (meta[\"n_unique\"]   >= FILTERS[\"min_unique\"])\n",
    ")\n",
    "drop_cols = list(meta.index[~good_mask])\n",
    "if drop_cols:\n",
    "    df.drop(columns=drop_cols, inplace=True)\n",
    "    logger.info(\"Dropped %d low-quality numeric columns\", len(drop_cols))\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 7 · COLLAPSE TO ONE ROW PER FIRM-YEAR\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "df[\"Year\"] = df[DATE_COL].dt.year\n",
    "df = (\n",
    "    df.sort_values(DATE_COL)\n",
    "      .groupby([ID_COL, \"Year\"], as_index=False)\n",
    "      .last()\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 8 · EXPORT & SUMMARY\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "out_csv = OUTPUT_DIR / \"stage01_cleaned.csv\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "\n",
    "buf = io.StringIO(); df.info(buf=buf)\n",
    "logger.info(\"Final DataFrame info:\\n%s\", buf.getvalue())\n",
    "logger.info(\"Saved cleaned CSV → %s\", out_csv)\n",
    "\n",
    "# Make result available in-memory if next stage imports in same session\n",
    "data_stage_1 = df.copy()\n",
    "logger.info(\"✅ STAGE 01 complete — `data_stage_1` ready\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
