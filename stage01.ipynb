{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af24517a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-13 13:35:59,885 | INFO    | ==========  STAGE 01: DATA LOAD & PRE-PROCESSING ==========\n",
      "2025-06-13 13:35:59,887 | INFO    | RUN_DIR        : outputs_rff\\daily\\2025-06-13\n",
      "2025-06-13 13:35:59,888 | INFO    | INPUT_CSV      : C:\\Users\\Jason Pohl\\OneDrive - Bond University\\PhD\\rff\\NEW_DATA.csv\n",
      "2025-06-13 13:35:59,891 | INFO    | SWAN_YEAR      : 2000  |  RUN_DATE: 2025-06-13\n",
      "2025-06-13 13:35:59,892 | INFO    | DATE_COL / ID_COL   = ReportDate / Symbol\n",
      "2025-06-13 13:35:59,894 | INFO    | FILTERS             = {'pct_non_na': 95, 'pct_zero': 98, 'min_unique': 10}\n",
      "2025-06-13 13:36:01,680 | INFO    | Rows loaded: 55,800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jason Pohl\\AppData\\Local\\Temp\\ipykernel_8172\\649719724.py:81: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S.%f format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  pd.to_datetime(df[DATE_COL], errors=\"coerce\", dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-13 13:36:02,275 | INFO    | After ID/date filter: 55800 rows (100.0%)\n",
      "2025-06-13 13:36:02,705 | INFO    | Dropped 94 low-quality numeric columns\n",
      "2025-06-13 13:36:05,925 | INFO    | Final DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34862 entries, 0 to 34861\n",
      "Columns: 164 entries, Symbol to ReportDate\n",
      "dtypes: datetime64[ns](1), float64(148), int32(1), int64(4), object(10)\n",
      "memory usage: 43.5+ MB\n",
      "\n",
      "2025-06-13 13:36:05,926 | INFO    | Saved cleaned CSV → outputs_rff\\daily\\2025-06-13\\stage01\\stage01_cleaned.csv\n",
      "2025-06-13 13:36:05,941 | INFO    | ✅ STAGE 01 complete — `data_stage_1` ready\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "STAGE 01 · DATA LOAD & PRE-PROCESSING\n",
    "────────────────────────────────────\n",
    "Reads the raw CSV, cleans it, and saves\n",
    "   outputs_rff/event=<YEAR>/<RUN_TAG>/stage01/stage01_cleaned.csv\n",
    "\n",
    "The orchestrator (or your own shell) must supply four ENV variables:\n",
    "  • INPUT_CSV    – full path to the raw data file\n",
    "  • OUTPUT_ROOT  – where the pipeline writes its outputs\n",
    "  • SWAN_YEAR    – the crisis year we are working on (2000 / 2008 / 2020 …)\n",
    "  • RUN_TAG      – any folder tag, usually today’s date (YYYY-MM-DD)\n",
    "\n",
    "Everything else (date column name, filters, etc.) is read from\n",
    "pipeline_config.yaml via pipeline_utils.load_cfg().\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import io, os, re, sys, logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pipeline_utils import load_cfg, resolve_run_dir\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 0 · CONFIG  &  RUN FOLDER   (Stage-01 only)\n",
    "# ───────────────────────────────────────────────\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from pipeline_utils import load_cfg\n",
    "\n",
    "CFG      = load_cfg()\n",
    "EVENTS   = {str(k): v for k, v in CFG.get(\"events\", {}).items()}   # NEW\n",
    "DEFAULTS = CFG.get(\"defaults\", {})\n",
    "\n",
    "# event & run-tag\n",
    "SWAN_YEAR = str(os.getenv(\"SWAN_YEAR\") or next(iter(EVENTS)))      # safe fallback\n",
    "RUN_TAG   = os.getenv(\"RUN_TAG\") or datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# paths\n",
    "OUTPUT_ROOT = Path(os.getenv(\"OUTPUT_ROOT\",\n",
    "                             DEFAULTS.get(\"OUTPUT_ROOT\", \"outputs_rff\")))\n",
    "RUN_DIR  = OUTPUT_ROOT / f\"event={SWAN_YEAR}\" / RUN_TAG            # ← we CREATE it\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INPUT_CSV  = Path(os.getenv(\"INPUT_CSV\", DEFAULTS.get(\"INPUT_CSV\", \"\"))).expanduser()\n",
    "OUTPUT_DIR = RUN_DIR / \"stage01\"; OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DATE_COL = DEFAULTS.get(\"DATE_COL\", \"ReportDate\")\n",
    "ID_COL   = DEFAULTS.get(\"ID_COL\",   \"Symbol\")\n",
    "FILTERS  = {\n",
    "    \"pct_non_na\": DEFAULTS.get(\"PCT_NON_NA\", 95),\n",
    "    \"pct_zero\":   DEFAULTS.get(\"PCT_ZERO\",   98),\n",
    "    \"min_unique\": DEFAULTS.get(\"MIN_UNIQUE\", 10),\n",
    "}\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 1 · LOGGER\n",
    "# ───────────────────────────────────────────────\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)-7s | %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(OUTPUT_DIR / \"stage01.log\", mode=\"w\", encoding=\"utf-8\"),\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ],\n",
    ")\n",
    "log = logging.getLogger(__name__)\n",
    "log.info(\"========== STAGE 01 ==========\")\n",
    "log.info(\"RUN_DIR   : %s\", RUN_DIR)\n",
    "log.info(\"INPUT_CSV : %s\", INPUT_CSV)\n",
    "log.info(\"SWAN_YEAR : %s  |  RUN_TAG: %s\", SWAN_YEAR, RUN_TAG)\n",
    "log.info(\"DATE / ID : %s / %s\", DATE_COL, ID_COL)\n",
    "log.info(\"FILTERS   : %s\", FILTERS)\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 2 · LOAD RAW DATA\n",
    "# ───────────────────────────────────────────────\n",
    "df = pd.read_csv(INPUT_CSV, low_memory=False)\n",
    "log.info(\"Rows loaded: %s\", f\"{len(df):,}\")\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 3 · DATE & ID CLEAN-UP\n",
    "# ───────────────────────────────────────────────\n",
    "df[DATE_COL] = (\n",
    "    pd.to_datetime(df[DATE_COL], errors=\"coerce\", dayfirst=True)\n",
    "      .fillna(pd.to_datetime(df[DATE_COL], errors=\"coerce\", dayfirst=False))\n",
    ")\n",
    "df[ID_COL]   = df[ID_COL].astype(str).str.strip().str.upper()\n",
    "\n",
    "bad_dates = df[df[DATE_COL].isna()]\n",
    "if not bad_dates.empty:\n",
    "    bad_dates.to_csv(OUTPUT_DIR / \"bad_dates.csv\", index=False)\n",
    "    log.warning(\"Bad dates → %d rows written to bad_dates.csv\", len(bad_dates))\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 4 · COERCE NUMERIC TEXT → FLOATS\n",
    "# ───────────────────────────────────────────────\n",
    "_num_rx = re.compile(r\"[$€£,%]\")\n",
    "def to_num(series: pd.Series) -> pd.Series:\n",
    "    if series.dtype != \"object\":\n",
    "        return series\n",
    "    out = pd.to_numeric(series.str.replace(_num_rx, \"\", regex=True), errors=\"coerce\")\n",
    "    # keep conversion only if ≥50 % values became numbers\n",
    "    return out if out.notna().mean() >= 0.50 else series\n",
    "\n",
    "df = df.apply(to_num)\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 5 · BASIC FILTERS\n",
    "# ───────────────────────────────────────────────\n",
    "before = len(df)\n",
    "df = df.dropna(subset=[ID_COL, DATE_COL])\n",
    "log.info(\"After ID/date filter: %d rows (%.1f %%)\", len(df), len(df)/before*100)\n",
    "\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "meta = pd.DataFrame({\n",
    "    \"pct_non_na\": df[num_cols].notna().mean()*100,\n",
    "    \"pct_zero\":   (df[num_cols]==0).mean()*100,\n",
    "    \"n_unique\":   df[num_cols].nunique(dropna=True),\n",
    "})\n",
    "good = (\n",
    "    (meta[\"pct_non_na\"] >= FILTERS[\"pct_non_na\"]) &\n",
    "    (meta[\"pct_zero\"]   <  FILTERS[\"pct_zero\"])   &\n",
    "    (meta[\"n_unique\"]   >= FILTERS[\"min_unique\"])\n",
    ")\n",
    "drop_cols = list(meta.index[~good])\n",
    "if drop_cols:\n",
    "    df = df.drop(columns=drop_cols)\n",
    "    log.info(\"Dropped %d noisy numeric columns\", len(drop_cols))\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 6 · KEEP LAST REPORT PER FIRM-YEAR\n",
    "# ───────────────────────────────────────────────\n",
    "df[\"Year\"] = df[DATE_COL].dt.year\n",
    "df = (\n",
    "    df.sort_values(DATE_COL)\n",
    "      .groupby([ID_COL, \"Year\"], as_index=False)\n",
    "      .last()\n",
    ")\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 7 · SAVE RESULT   (patched: add _<SWAN_YEAR>)\n",
    "# ───────────────────────────────────────────────\n",
    "out_csv = OUTPUT_DIR / f\"stage01_cleaned_{SWAN_YEAR}.csv\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "\n",
    "buf = io.StringIO(); df.info(buf=buf)\n",
    "log.info(\"Final DataFrame info:\\n%s\", buf.getvalue())\n",
    "log.info(\"Saved cleaned CSV → %s\", out_csv)\n",
    "log.info(\"✅ STAGE 01 complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
