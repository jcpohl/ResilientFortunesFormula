{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af24517a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:36:57,957 | INFO    | [utils] ========== STAGE 01 ==========\n",
      "2025-06-18 11:36:57,958 | INFO    | [utils] Run dir      : C:\\Users\\Jason Pohl\\OneDrive - Bond University\\PhD\\rff\\outputs_rff\\event=2008\\2025-06-18\n",
      "2025-06-18 11:36:57,960 | INFO    | [utils] Input CSV    : C:\\Users\\Jason Pohl\\OneDrive - Bond University\\PhD\\rff\\NEW_DATA.csv\n",
      "2025-06-18 11:36:57,961 | INFO    | [utils] Save format  : csv\n",
      "2025-06-18 11:36:57,962 | INFO    | [utils] DATE / ID    : ReportDate / Symbol\n",
      "2025-06-18 11:36:57,963 | INFO    | [utils] Filters      : {'pct_non_na': 95, 'pct_zero': 98, 'min_unique': 10}\n",
      "2025-06-18 11:37:00,336 | INFO    | [utils] Rows loaded  : 55,800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jason Pohl\\AppData\\Local\\Temp\\ipykernel_23876\\1031405712.py:89: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S.%f format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  pd.to_datetime(df[DATE_COL], errors=\"coerce\", dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 11:37:01,086 | INFO    | [utils] After ID/date filter: 55800 rows (100.0% kept)\n",
      "2025-06-18 11:37:01,617 | INFO    | [utils] Dropped 94 noisy numeric columns\n",
      "2025-06-18 11:37:05,152 | INFO    | [utils] Final DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34862 entries, 0 to 34861\n",
      "Columns: 164 entries, Symbol to ReportDate\n",
      "dtypes: datetime64[ns](1), float64(148), int32(1), int64(4), object(10)\n",
      "memory usage: 43.5+ MB\n",
      "\n",
      "2025-06-18 11:37:05,152 | INFO    | [utils] Saved cleaned file → stage01_cleaned_2008.csv\n",
      "2025-06-18 11:37:05,153 | INFO    | [utils] ✅ STAGE 01 complete\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "STAGE 01 · DATA LOAD & PRE-PROCESSING\n",
    "─────────────────────────────────────\n",
    "Reads the raw CSV, cleans it, and saves either CSV **or** Parquet\n",
    "(depending on the SAVE_FORMAT flag passed from the orchestrator).\n",
    "\n",
    "Outputs\n",
    "-------\n",
    "<OUTPUT_ROOT>/event=<YEAR>/<RUN_TAG>/stage01/\n",
    "    └─ stage01_cleaned_<YEAR>.(csv|parquet)\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import io, logging, os, re, sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pipeline_utils import load_cfg\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 0 · CONFIG  &  RUN FOLDER   (Stage-01 only)\n",
    "# ───────────────────────────────────────────────\n",
    "CFG      = load_cfg()\n",
    "EVENTS   = {str(k): v for k, v in CFG.get(\"events\", {}).items()}\n",
    "DEFAULTS = CFG.get(\"defaults\", {})\n",
    "\n",
    "SWAN_YEAR = str(os.getenv(\"SWAN_YEAR\") or next(iter(EVENTS)))\n",
    "RUN_TAG   = os.getenv(\"RUN_TAG\") or datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# ----- OUTPUT_ROOT (env ▸ yaml lower ▸ yaml upper ▸ fallback) -----\n",
    "OUTPUT_ROOT = Path(\n",
    "    os.getenv(\n",
    "        \"OUTPUT_ROOT\",\n",
    "        DEFAULTS.get(\"output_root\",\n",
    "                     DEFAULTS.get(\"OUTPUT_ROOT\", \"outputs_rff\"))\n",
    "    )\n",
    ").expanduser()\n",
    "\n",
    "RUN_DIR = OUTPUT_ROOT / f\"event={SWAN_YEAR}\" / RUN_TAG\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)          # create parent dirs\n",
    "\n",
    "# stage-specific output folder **must exist before logger config**\n",
    "OUTPUT_DIR = RUN_DIR / \"stage01\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "INPUT_CSV  = Path(os.getenv(\"INPUT_CSV\",\n",
    "                            DEFAULTS.get(\"INPUT_CSV\", \"\"))).expanduser()\n",
    "\n",
    "DATE_COL = DEFAULTS.get(\"DATE_COL\", \"ReportDate\")\n",
    "ID_COL   = DEFAULTS.get(\"ID_COL\",   \"Symbol\")\n",
    "FILTERS  = {\n",
    "    \"pct_non_na\": DEFAULTS.get(\"PCT_NON_NA\", 95),\n",
    "    \"pct_zero\":   DEFAULTS.get(\"PCT_ZERO\",   98),\n",
    "    \"min_unique\": DEFAULTS.get(\"MIN_UNIQUE\", 10),\n",
    "}\n",
    "SAVE_FORMAT: Literal[\"csv\", \"parquet\"] = (\n",
    "    os.getenv(\"SAVE_FORMAT\", DEFAULTS.get(\"SAVE_FORMAT\", \"csv\")).lower()\n",
    ")\n",
    "\n",
    "\n",
    "# ─────────────────────────── 1 · LOGGER ────────────────────────────\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)-7s | %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(OUTPUT_DIR / \"stage01.log\", \"w\", \"utf-8\"),\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ],\n",
    ")\n",
    "log = logging.getLogger(__name__)\n",
    "log.info(\"========== STAGE 01 ==========\")\n",
    "log.info(\"Run dir      : %s\", RUN_DIR)\n",
    "log.info(\"Input CSV    : %s\", INPUT_CSV)\n",
    "log.info(\"Save format  : %s\", SAVE_FORMAT)\n",
    "log.info(\"DATE / ID    : %s / %s\", DATE_COL, ID_COL)\n",
    "log.info(\"Filters      : %s\", FILTERS)\n",
    "\n",
    "# ─────────────────────────── 2 · LOAD RAW DATA ─────────────────────\n",
    "df = pd.read_csv(INPUT_CSV, low_memory=False)\n",
    "log.info(\"Rows loaded  : %s\", f\"{len(df):,}\")\n",
    "\n",
    "# ─────────────────────────── 3 · DATE & ID CLEAN-UP ────────────────\n",
    "df[DATE_COL] = (\n",
    "    pd.to_datetime(df[DATE_COL], errors=\"coerce\", dayfirst=True)\n",
    "      .fillna(pd.to_datetime(df[DATE_COL], errors=\"coerce\", dayfirst=False))\n",
    ")\n",
    "df[ID_COL] = df[ID_COL].astype(str).str.strip().str.upper()\n",
    "\n",
    "bad_dates = df[df[DATE_COL].isna()]\n",
    "if not bad_dates.empty:\n",
    "    bad_dates.to_csv(OUTPUT_DIR / \"bad_dates.csv\", index=False)\n",
    "    log.warning(\"Bad dates → %d rows written to bad_dates.csv\", len(bad_dates))\n",
    "\n",
    "# ─────────────────────────── 4 · COERCE TEXT-NUMBERS ───────────────\n",
    "_num_rx = re.compile(r\"[$€£,%]\")\n",
    "def to_num(s: pd.Series) -> pd.Series:\n",
    "    if s.dtype != \"object\":\n",
    "        return s\n",
    "    out = pd.to_numeric(s.str.replace(_num_rx, \"\", regex=True), errors=\"coerce\")\n",
    "    return out if out.notna().mean() >= 0.50 else s\n",
    "\n",
    "df = df.apply(to_num)\n",
    "\n",
    "# ─────────────────────────── 5 · BASIC FILTERS ─────────────────────\n",
    "before = len(df)\n",
    "df = df.dropna(subset=[ID_COL, DATE_COL])\n",
    "log.info(\"After ID/date filter: %d rows (%.1f%% kept)\", len(df), len(df)/before*100)\n",
    "\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "meta = pd.DataFrame({\n",
    "    \"pct_non_na\": df[num_cols].notna().mean()*100,\n",
    "    \"pct_zero\":   (df[num_cols] == 0).mean()*100,\n",
    "    \"n_unique\":   df[num_cols].nunique(dropna=True),\n",
    "})\n",
    "good = (\n",
    "    (meta[\"pct_non_na\"] >= FILTERS[\"pct_non_na\"]) &\n",
    "    (meta[\"pct_zero\"]   <  FILTERS[\"pct_zero\"])   &\n",
    "    (meta[\"n_unique\"]   >= FILTERS[\"min_unique\"])\n",
    ")\n",
    "drop_cols = list(meta.index[~good])\n",
    "if drop_cols:\n",
    "    df = df.drop(columns=drop_cols)\n",
    "    log.info(\"Dropped %d noisy numeric columns\", len(drop_cols))\n",
    "\n",
    "# ─────────────────────────── 6 · ONE REPORT PER FIRM-YEAR ──────────\n",
    "df[\"Year\"] = df[DATE_COL].dt.year\n",
    "df = (\n",
    "    df.sort_values(DATE_COL)\n",
    "      .groupby([ID_COL, \"Year\"], as_index=False)\n",
    "      .last()\n",
    ")\n",
    "\n",
    "# ─────────────────────────── 7 · SAVE RESULT ───────────────────────\n",
    "out_file = OUTPUT_DIR / f\"stage01_cleaned_{SWAN_YEAR}.{SAVE_FORMAT}\"\n",
    "if SAVE_FORMAT == \"parquet\":\n",
    "    df.to_parquet(out_file, index=False)\n",
    "else:\n",
    "    df.to_csv(out_file, index=False)\n",
    "\n",
    "buf = io.StringIO(); df.info(buf=buf)\n",
    "log.info(\"Final DataFrame info:\\n%s\", buf.getvalue())\n",
    "log.info(\"Saved cleaned file → %s\", out_file.name)\n",
    "log.info(\"✅ STAGE 01 complete\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
