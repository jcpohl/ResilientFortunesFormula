{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6675f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# ===================================================================\n",
    "#  STAGE-25 · UNIVERSAL RATIO-vs-RESILIENCE RANKER   v4.3 · 2025-06-20\n",
    "# ===================================================================\n",
    "\"\"\"\n",
    " * Works with the upgraded pipeline (Stages 02-07).\n",
    " * Accepts any subset of the four outcome families; families whose\n",
    "   score/flag columns are missing are skipped automatically.\n",
    " * Uses keyword args with `resolve_run_dir` (v2.1+) – no TypeErrors.\n",
    " * Gracefully handles events that lack Dynamic flags.\n",
    " * Logging now honours `stage25/` folder just like other stages.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import contextlib, io, logging, math, os, sys, warnings\n",
    "from pathlib import Path\n",
    "from typing  import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats     import chi2, norm, spearmanr\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from numpy.linalg    import LinAlgError\n",
    "from statsmodels.tools.sm_exceptions import (\n",
    "    MissingDataError, PerfectSeparationError,\n",
    ")\n",
    "\n",
    "from pipeline_utils import load_cfg, resolve_run_dir\n",
    "\n",
    "# ═════════════════════ GLOBALS ══════════════════════\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "CFG         = load_cfg()\n",
    "EVENT_LIST  = list(CFG.get(\"events\", {}).keys())            # run every event folder\n",
    "\n",
    "MIN_COVER   = float(os.getenv(\"MIN_COVERAGE\", 40))          # % non-NA\n",
    "MIN_OBS     = int  (os.getenv(\"MIN_OBS\", 30))               # minimum rows for rho / logit\n",
    "EPS_VAR     = 1e-6                                          # flat-series guard\n",
    "\n",
    "ID_COL, YEAR_COL, DATE_COL = \"Symbol\", \"Year\", \"ReportDate\"\n",
    "_IN_NOTEBOOK = \"ipykernel\" in sys.modules\n",
    "\n",
    "# ── logger (one file per run root) ─────────────────────────────────\n",
    "ROOT_RUN = None\n",
    "try:\n",
    "    # choose the first event that actually exists so the log ends up in a run folder\n",
    "    ROOT_RUN = resolve_run_dir(\n",
    "        swan_year = EVENT_LIST[0], run_tag = os.getenv(\"RUN_TAG\"), create = False\n",
    "    ).parent\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "LOG_PATH = (ROOT_RUN / \"stage25\" / \"stage25.log\") if ROOT_RUN else None\n",
    "if LOG_PATH:\n",
    "    LOG_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level   = logging.INFO,\n",
    "    format  = \"%(asctime)s | %(levelname)-7s | %(message)s\",\n",
    "    handlers=[logging.FileHandler(LOG_PATH, \"w\", \"utf-8\")] if LOG_PATH else [],\n",
    ")\n",
    "log = logging.getLogger(\"stage25\")\n",
    "\n",
    "\n",
    "# ═════════════════════ STATS HELPERS ════════════════\n",
    "def _safe_spearman(x: pd.Series, y: pd.Series):\n",
    "    ok = x.notna() & y.notna()\n",
    "    if ok.sum() < MIN_OBS or x[ok].var() < EPS_VAR or y[ok].var() < EPS_VAR:\n",
    "        return np.nan, np.nan\n",
    "    return spearmanr(x[ok], y[ok])[:2]\n",
    "\n",
    "def _logit_one(x: pd.Series, flag: pd.Series) -> Dict:\n",
    "    ok = x.notna() & flag.isin([0, 1])\n",
    "    n  = int(ok.sum())\n",
    "    res = dict(SampleSize=n, PositivePct=float(flag[ok].mean()*100) if n else np.nan,\n",
    "               PseudoR2=np.nan, AUROC=np.nan, CoefP=np.nan,\n",
    "               ModelConv=False, Failure=\"\")\n",
    "    if n < MIN_OBS:                     res[\"Failure\"] = \"too_few_obs\";  return res\n",
    "    if flag[ok].nunique() < 2:          res[\"Failure\"] = \"single_class\"; return res\n",
    "    if x[ok].var() < EPS_VAR:           res[\"Failure\"] = \"zero_variance\";return res\n",
    "    try:\n",
    "        mdl = sm.Logit(flag[ok], sm.add_constant(x[ok])).fit(disp=False)\n",
    "        if not mdl.mle_retvals.get(\"converged\", True):\n",
    "            res[\"Failure\"] = \"no_convergence\"; return res\n",
    "        res.update(ModelConv=True,\n",
    "                   PseudoR2=float(mdl.prsquared),\n",
    "                   AUROC=float(roc_auc_score(flag[ok], mdl.predict())),\n",
    "                   CoefP=float(mdl.pvalues.iloc[1]))\n",
    "    except (ValueError, LinAlgError, MissingDataError, PerfectSeparationError):\n",
    "        res[\"Failure\"] = \"model_error\"\n",
    "    return res\n",
    "\n",
    "def _fisher(p: List[float])  -> float:      # meta-p\n",
    "    g = [q for q in p if 0 < q < 1]\n",
    "    if not g: return np.nan\n",
    "    stat = -2 * sum(math.log(max(q, 1e-300)) for q in g)\n",
    "    return 1 - chi2.cdf(stat, 2*len(g))\n",
    "\n",
    "def _stouffer(p: List[float]) -> float:     # meta-z\n",
    "    g = [q for q in p if 0 < q < 1]\n",
    "    if not g: return np.nan\n",
    "    z = [norm.isf(q/2) * math.copysign(1, .5-q) for q in g]\n",
    "    return sum(z)/math.sqrt(len(z))\n",
    "\n",
    "# ═════════════════════ RATIO DETECTOR ═══════════════\n",
    "def _ratio_columns(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Returns (raw_like, winsor_like) lists.\n",
    "    *_raw*  or *_orig* = un-winsorised; the partner column (same stem) is winsor.\n",
    "    \"\"\"\n",
    "    if any(c.endswith(\"_raw\") for c in df.columns):\n",
    "        raw  = [c for c in df if c.endswith(\"_raw\")  and c[:-4] in df.columns]\n",
    "        wins = [c[:-4] for c in raw]\n",
    "    elif any(c.endswith(\"_orig\") for c in df.columns):\n",
    "        raw  = [c for c in df if c.endswith(\"_orig\") and c[:-5] in df.columns]\n",
    "        wins = [c[:-5] for c in raw]\n",
    "    else:                                     # already only one copy (winsor)\n",
    "        raw, wins = [], [c for c in df if \"_\" in c]\n",
    "    return raw, wins\n",
    "\n",
    "\n",
    "# ═════════════════════ ONE-EVENT CORE ═══════════════\n",
    "def run_event(ev: str) -> pd.DataFrame:\n",
    "    run_dir = resolve_run_dir(\n",
    "        swan_year = ev,\n",
    "        run_tag   = os.getenv(\"RUN_TAG\"),\n",
    "        must_have = f\"stage03/Stage3_Data_WithRatios_{ev}.csv\",\n",
    "    )\n",
    "    out_dir = run_dir / \"stage25\"\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(run_dir/\"stage03\"/f\"Stage3_Data_WithRatios_{ev}.csv\")\n",
    "    if YEAR_COL not in df and DATE_COL in df:\n",
    "        df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "        df[YEAR_COL] = df[DATE_COL].dt.year\n",
    "    df = df[df[YEAR_COL] < int(ev)].copy()           # pre-event only\n",
    "\n",
    "    raw_cols, win_cols = _ratio_columns(df)\n",
    "\n",
    "    # outcome families -----------------------------------------------------\n",
    "    OUT = {\n",
    "        \"Temporal\": dict(score=\"ScoreTemporal_\", flag=\"FlagTemporal_\"),\n",
    "        \"Impact\"  : dict(score=\"ScoreImpact_\",   flag=\"FlagImpact_\"),\n",
    "        \"Dynamic\" : dict(score=\"ScoreDynamic_\",  flag=\"FlagDynamic_\"),\n",
    "        \"Blend\"   : dict(score=\"ScoreBlend_\",    flag=\"FlagBlend_\"),\n",
    "    }\n",
    "\n",
    "    # available metrics per family\n",
    "    metric_map: Dict[str, List[str]] = {}\n",
    "    for flav, pre in OUT.items():\n",
    "        score_mets = {c[len(pre[\"score\"]):] for c in df if c.startswith(pre[\"score\"])}\n",
    "        flag_mets  = {c[len(pre[\"flag\"] ):] for c in df if c.startswith(pre[\"flag\"] )}\n",
    "        metric_map[flav] = sorted(score_mets & flag_mets)\n",
    "\n",
    "    # ranking routine ------------------------------------------------------\n",
    "    def _rank(cols: list[str], flavour: str) -> pd.DataFrame:\n",
    "        if not metric_map[flavour]:                       # family absent → skip\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        pre = OUT[flavour]\n",
    "        recs = []\n",
    "        for ratio in cols:\n",
    "            cov = df[ratio].notna().mean()*100\n",
    "            if cov < MIN_COVER:\n",
    "                continue\n",
    "            for m in metric_map[flavour]:\n",
    "                rho, p_rho = _safe_spearman(df[ratio], df[f\"{pre['score']}{m}\"])\n",
    "                recs.append(dict(\n",
    "                    SwanYear    = int(ev),\n",
    "                    Ratio       = ratio.replace(\"_raw\",\"\").replace(\"_orig\",\"\"),\n",
    "                    Metric      = m,\n",
    "                    CoveragePct = round(cov,1),\n",
    "                    AbsRho      = abs(rho) if pd.notna(rho) else np.nan,\n",
    "                    RhoP        = p_rho,\n",
    "                    **_logit_one(df[ratio], df[f\"{pre['flag']}{m}\"]),\n",
    "                    Flavour     = flavour,\n",
    "                    Series      = \"winsor\" if ratio in win_cols else \"raw\",\n",
    "                ))\n",
    "        res = pd.DataFrame(recs)\n",
    "        if not res.empty:\n",
    "            tag = \"winsor\" if cols is win_cols else \"raw\"\n",
    "            res.to_csv(out_dir / f\"Stage25_{tag}_{flavour}_RatioRanking_{ev}.csv\",\n",
    "                       index=False)\n",
    "        return res\n",
    "\n",
    "    frames = []\n",
    "    if win_cols: frames += [_rank(win_cols, f) for f in OUT]\n",
    "    if raw_cols: frames += [_rank(raw_cols, f) for f in OUT]\n",
    "\n",
    "    out_df = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "    log.info(\"Stage-25 %s → %d rows\", ev, len(out_df))\n",
    "    return out_df\n",
    "\n",
    "\n",
    "# ═════════════════════ DRIVER ═══════════════════════\n",
    "def main():\n",
    "    all_frames = []\n",
    "    for ev in EVENT_LIST:\n",
    "        try:\n",
    "            all_frames.append(run_event(ev))\n",
    "        except Exception as exc:\n",
    "            log.error(\"Event %s skipped: %s\", ev, exc)\n",
    "\n",
    "    if not all_frames:\n",
    "        log.warning(\"No events processed – nothing to summarise\")\n",
    "        return\n",
    "\n",
    "    big = pd.concat(all_frames, ignore_index=True)\n",
    "    sel = big[(big.Series == \"winsor\") & big.ModelConv]\n",
    "\n",
    "    if sel.empty:\n",
    "        log.warning(\"No converged winsor models – meta summary skipped\")\n",
    "        return\n",
    "\n",
    "    meta = (sel.groupby([\"Ratio\", \"Metric\", \"Flavour\"])\n",
    "              .apply(lambda g: pd.Series(dict(\n",
    "                  meanAbsRho = g.AbsRho.mean(),\n",
    "                  meanAUROC  = g.AUROC.mean(),\n",
    "                  FisherP    = _fisher(g.CoefP.dropna().tolist()),\n",
    "                  StoufferZ  = _stouffer(g.CoefP.dropna().tolist()),\n",
    "                  nEventsSig = int((g.CoefP < .05).sum()),\n",
    "                  Events     = \",\".join(map(str, sorted(g.SwanYear.unique()))),\n",
    "              ))).reset_index()\n",
    "              .sort_values([\"Flavour\", \"FisherP\", \"meanAUROC\"]))\n",
    "\n",
    "    # write inside *latest* run folder\n",
    "    last_ev = next((e for e in reversed(EVENT_LIST)\n",
    "                    if (resolve_run_dir(swan_year=e, run_tag=os.getenv(\"RUN_TAG\"),\n",
    "                                        create=False, must_have=\"\")).exists()), None)\n",
    "    if last_ev is None:\n",
    "        log.warning(\"Run folder not found – meta summary NOT written\")\n",
    "        return\n",
    "\n",
    "    meta_dir = resolve_run_dir(swan_year=last_ev, run_tag=os.getenv(\"RUN_TAG\"))\n",
    "    (meta_dir / \"stage25\").mkdir(exist_ok=True)\n",
    "    out_fp = meta_dir / \"stage25\" / \"Stage25_CrossEvent_Summary.csv\"\n",
    "    meta.to_csv(out_fp, index=False)\n",
    "    log.info(\"Cross-event summary saved → %s\", out_fp)\n",
    "\n",
    "    if _IN_NOTEBOOK:\n",
    "        from IPython.display import display, HTML\n",
    "        display(HTML(\"<h3>Stage-25 cross-event summary (top 50)</h3>\"))\n",
    "        display(meta.head(50).style.background_gradient(cmap=\"Purples\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # hide noisy stdout from statsmodels when run at the CLI\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
