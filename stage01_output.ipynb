{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4bbc266",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T04:39:49.469899Z",
     "iopub.status.busy": "2025-06-11T04:39:49.469899Z",
     "iopub.status.idle": "2025-06-11T04:39:49.480893Z",
     "shell.execute_reply": "2025-06-11T04:39:49.480893Z"
    },
    "papermill": {
     "duration": 0.024628,
     "end_time": "2025-06-11T04:39:49.490008",
     "exception": false,
     "start_time": "2025-06-11T04:39:49.465380",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "INPUT_CSV = \"C:/Users/Jason Pohl/OneDrive - Bond University/PhD/rff/NEW_DATA.csv\"\n",
    "OUTPUT_ROOT = \"C:/Users/Jason Pohl/OneDrive - Bond University/PhD/rff/outputs_rff\"\n",
    "STAGE1_CFG = \"\"\n",
    "SWAN_YEAR = 2008\n",
    "WIN_START = 2004\n",
    "WIN_END = 2012\n",
    "RUN_TAG = \"myUniqueRunId\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af24517a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T04:39:49.496442Z",
     "iopub.status.busy": "2025-06-11T04:39:49.495441Z",
     "iopub.status.idle": "2025-06-11T04:39:57.451378Z",
     "shell.execute_reply": "2025-06-11T04:39:57.450379Z"
    },
    "papermill": {
     "duration": 7.961284,
     "end_time": "2025-06-11T04:39:57.453382",
     "exception": false,
     "start_time": "2025-06-11T04:39:49.492098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing RUN_DIR: outputs_rff\\daily\\2025-06-11\n",
      "2025-06-11 14:39:50,362 | INFO    | ==========  STAGE 01: DATA LOAD & PRE-PROCESSING ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:39:50,364 | INFO    | RUN_DIR        : outputs_rff\\daily\\2025-06-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:39:50,365 | INFO    | INPUT_CSV      : C:\\Users\\Jason Pohl\\OneDrive - Bond University\\PhD\\rff\\NEW_DATA.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:39:50,367 | INFO    | SWAN_YEAR      : 2008  |  RUN_DATE: 2025-06-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:39:50,367 | INFO    | DATE_COL / ID_COL   = ReportDate / Symbol\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:39:50,368 | INFO    | FILTERS             = {'pct_non_na': 95, 'pct_zero': 98, 'min_unique': 10}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:39:52,435 | INFO    | Rows loaded: 55,800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jason Pohl\\AppData\\Local\\Temp\\ipykernel_35720\\649719724.py:81: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S.%f format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  pd.to_datetime(df[DATE_COL], errors=\"coerce\", dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:39:53,107 | INFO    | After ID/date filter: 55800 rows (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:39:53,562 | INFO    | Dropped 94 low-quality numeric columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:39:57,418 | INFO    | Final DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34862 entries, 0 to 34861\n",
      "Columns: 164 entries, Symbol to ReportDate\n",
      "dtypes: datetime64[ns](1), float64(148), int32(1), int64(4), object(10)\n",
      "memory usage: 43.5+ MB\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:39:57,419 | INFO    | Saved cleaned CSV → outputs_rff\\daily\\2025-06-11\\stage01\\stage01_cleaned.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:39:57,438 | INFO    | ✅ STAGE 01 complete — `data_stage_1` ready\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STAGE 01 · DATA LOAD & PRE-PROCESSING\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "• Reads run parameters from pipeline_config.yaml\n",
    "• If SWAN_YEAR is not supplied via env-var, defaults to the first event\n",
    "  key in the YAML, so the script can run with *zero* manual input.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os, logging, io, re, yaml, sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import daily_setup  # This will set up RUN_DIR for today\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 0-2 · UNIVERSAL BOOTSTRAP  (cfg + run-path resolver + logger)\n",
    "#       uses pipeline_utils.py that you added earlier\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "from pipeline_utils import load_cfg, resolve_run_dir    # <— NEW\n",
    "import logging, sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os, io, re\n",
    "\n",
    "# 0.1 read YAML once\n",
    "CFG       = load_cfg()\n",
    "DEFAULTS  = CFG.get(\"defaults\", {})\n",
    "EVENTS    = CFG.get(\"events\", {})\n",
    "\n",
    "# 0.2 decide event (swan year) and resolve run directory\n",
    "SWAN_YEAR = str(os.getenv(\"SWAN_YEAR\") or next(iter(EVENTS)))\n",
    "RUN_DIR   = resolve_run_dir()          # OUTPUT_ROOT/event=YYYY/<run-tag>/\n",
    "\n",
    "RUN_DATE  = RUN_DIR.name               # folder name itself\n",
    "INPUT_CSV = Path(DEFAULTS[\"INPUT_CSV\"]).expanduser()\n",
    "\n",
    "# 0.3 misc constants\n",
    "DATE_COL  = DEFAULTS.get(\"DATE_COL\", \"ReportDate\")\n",
    "ID_COL    = DEFAULTS.get(\"ID_COL\",   \"Symbol\")\n",
    "FILTERS   = {\n",
    "    \"pct_non_na\": DEFAULTS.get(\"PCT_NON_NA\", 95),\n",
    "    \"pct_zero\":   DEFAULTS.get(\"PCT_ZERO\",   98),\n",
    "    \"min_unique\": DEFAULTS.get(\"MIN_UNIQUE\", 10),\n",
    "}\n",
    "\n",
    "# 1. output directory for this stage\n",
    "OUTPUT_DIR = RUN_DIR / \"stage01\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)-7s | %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(OUTPUT_DIR / \"stage01.log\", mode=\"w\", encoding=\"utf-8\"),\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ],\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"==========  STAGE 01: DATA LOAD & PRE-PROCESSING ==========\")\n",
    "logger.info(\"RUN_DIR        : %s\", RUN_DIR)\n",
    "logger.info(\"INPUT_CSV      : %s\", INPUT_CSV)\n",
    "logger.info(\"SWAN_YEAR      : %s  |  RUN_DATE: %s\", SWAN_YEAR, RUN_DATE)\n",
    "logger.info(\"DATE_COL / ID_COL   = %s / %s\", DATE_COL, ID_COL)\n",
    "logger.info(\"FILTERS             = %s\", FILTERS)\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 3 · LOAD RAW CSV\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "df = pd.read_csv(INPUT_CSV, low_memory=False)\n",
    "logger.info(\"Rows loaded: %s\", f\"{len(df):,}\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 4 · DATE PARSE & ID NORMALISATION\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "df[DATE_COL] = (\n",
    "    pd.to_datetime(df[DATE_COL], errors=\"coerce\", dayfirst=True)\n",
    "      .fillna(pd.to_datetime(df[DATE_COL], errors=\"coerce\", dayfirst=False))\n",
    ")\n",
    "bad_dates = df[df[DATE_COL].isna()]\n",
    "if not bad_dates.empty:\n",
    "    bad_dates.to_csv(OUTPUT_DIR / \"bad_dates.csv\", index=False)\n",
    "    logger.warning(\"Bad dates → %d rows saved to bad_dates.csv\", len(bad_dates))\n",
    "\n",
    "df[ID_COL] = df[ID_COL].astype(str).str.strip().str.upper()\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 5 · SMART NUMERIC COERCION\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "_num_rx = re.compile(r\"[$€£,%]\")\n",
    "\n",
    "def smart_to_num(series: pd.Series) -> pd.Series:\n",
    "    if series.dtype != \"object\":\n",
    "        return series\n",
    "    out = pd.to_numeric(series.str.replace(_num_rx, \"\", regex=True), errors=\"coerce\")\n",
    "    return out if out.notna().mean() >= 0.50 else series\n",
    "\n",
    "df = df.apply(smart_to_num)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 6 · BASIC FILTERS\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "before = len(df)\n",
    "df = df.dropna(subset=[ID_COL, DATE_COL])\n",
    "logger.info(\"After ID/date filter: %d rows (%.1f%%)\", len(df), len(df) / before * 100)\n",
    "\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "meta = pd.DataFrame({\n",
    "    \"pct_non_na\": df[num_cols].notna().mean() * 100,\n",
    "    \"pct_zero\":   (df[num_cols] == 0).mean() * 100,\n",
    "    \"n_unique\":   df[num_cols].nunique(dropna=True),\n",
    "})\n",
    "good_mask = (\n",
    "    (meta[\"pct_non_na\"] >= FILTERS[\"pct_non_na\"]) &\n",
    "    (meta[\"pct_zero\"]   <  FILTERS[\"pct_zero\"])   &\n",
    "    (meta[\"n_unique\"]   >= FILTERS[\"min_unique\"])\n",
    ")\n",
    "drop_cols = list(meta.index[~good_mask])\n",
    "if drop_cols:\n",
    "    df.drop(columns=drop_cols, inplace=True)\n",
    "    logger.info(\"Dropped %d low-quality numeric columns\", len(drop_cols))\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 7 · COLLAPSE TO ONE ROW PER FIRM-YEAR\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "df[\"Year\"] = df[DATE_COL].dt.year\n",
    "df = (\n",
    "    df.sort_values(DATE_COL)\n",
    "      .groupby([ID_COL, \"Year\"], as_index=False)\n",
    "      .last()\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 8 · EXPORT & SUMMARY\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "out_csv = OUTPUT_DIR / \"stage01_cleaned.csv\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "\n",
    "buf = io.StringIO(); df.info(buf=buf)\n",
    "logger.info(\"Final DataFrame info:\\n%s\", buf.getvalue())\n",
    "logger.info(\"Saved cleaned CSV → %s\", out_csv)\n",
    "\n",
    "# Make result available in-memory if next stage imports in same session\n",
    "data_stage_1 = df.copy()\n",
    "logger.info(\"✅ STAGE 01 complete — `data_stage_1` ready\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.610033,
   "end_time": "2025-06-11T04:39:57.791652",
   "environment_variables": {},
   "exception": null,
   "input_path": "stage01.ipynb",
   "output_path": "stage01_output.ipynb",
   "parameters": {
    "INPUT_CSV": "C:/Users/Jason Pohl/OneDrive - Bond University/PhD/rff/NEW_DATA.csv",
    "OUTPUT_ROOT": "C:/Users/Jason Pohl/OneDrive - Bond University/PhD/rff/outputs_rff",
    "RUN_TAG": "myUniqueRunId",
    "STAGE1_CFG": "",
    "SWAN_YEAR": 2008,
    "WIN_END": 2012,
    "WIN_START": 2004
   },
   "start_time": "2025-06-11T04:39:48.181619",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}