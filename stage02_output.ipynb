{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce87485f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T04:40:00.850112Z",
     "iopub.status.busy": "2025-06-11T04:40:00.850112Z",
     "iopub.status.idle": "2025-06-11T04:40:00.862219Z",
     "shell.execute_reply": "2025-06-11T04:40:00.861123Z"
    },
    "papermill": {
     "duration": 0.017622,
     "end_time": "2025-06-11T04:40:00.864243",
     "exception": false,
     "start_time": "2025-06-11T04:40:00.846621",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "INPUT_CSV = \"C:/Users/Jason Pohl/OneDrive - Bond University/PhD/rff/NEW_DATA.csv\"\n",
    "OUTPUT_ROOT = \"C:/Users/Jason Pohl/OneDrive - Bond University/PhD/rff/outputs_rff\"\n",
    "STAGE1_CFG = \"\"\n",
    "SWAN_YEAR = 2008\n",
    "WIN_START = 2004\n",
    "WIN_END = 2012\n",
    "RUN_TAG = \"myUniqueRunId\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fafcc2dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T04:40:00.869485Z",
     "iopub.status.busy": "2025-06-11T04:40:00.869485Z",
     "iopub.status.idle": "2025-06-11T04:40:19.702552Z",
     "shell.execute_reply": "2025-06-11T04:40:19.701555Z"
    },
    "papermill": {
     "duration": 18.838314,
     "end_time": "2025-06-11T04:40:19.704551",
     "exception": false,
     "start_time": "2025-06-11T04:40:00.866237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:40:01,219 | INFO    | ==========  STAGE 2: RESILIENCE METRICS ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:40:01,221 | INFO    | RUN_DIR        : outputs_rff\\daily\\2025-06-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:40:01,222 | INFO    | SWAN_YEAR=2008  RUN_DATE=2025-06-11  MAX_YEARS=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:40:01,224 | INFO    | DATE_COL / ID_COL = ReportDate / Symbol\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:40:02,305 | INFO    | Stage 1 CSV loaded: 34,862 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:40:03,376 | INFO    | NetIncome                      baseline 48.9% | recovery 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:40:04,520 | INFO    | EarningBeforeInterestAndTax    baseline 48.9% | recovery 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:40:05,636 | INFO    | OperatingIncome                baseline 48.9% | recovery 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:40:06,877 | INFO    | EBITDA                         baseline 48.9% | recovery 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:40:08,267 | INFO    | OperatingCashFlow              baseline 48.9% | recovery 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:40:09,681 | INFO    | FreeCashFlow                   baseline 48.9% | recovery 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:40:11,011 | INFO    | Cash                           baseline 48.9% | recovery 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:40:12,291 | INFO    | CashAndCashEquivalents         baseline 48.9% | recovery 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:40:13,655 | INFO    | TotalRevenue                   baseline 48.9% | recovery 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:40:14,927 | INFO    | GrossProfit                    baseline 48.9% | recovery 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:40:19,631 | INFO    | Final DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34862 entries, 0 to 34861\n",
      "Columns: 196 entries, Symbol to Flag_GrossProfit\n",
      "dtypes: Int16(1), datetime64[ns](1), float64(180), int64(4), object(10)\n",
      "memory usage: 52.0+ MB\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:40:19,632 | INFO    | Saved Stage 2 CSV → outputs_rff\\daily\\2025-06-11\\stage02\\Stage2_Data_WithMetrics.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:40:19,696 | INFO    | ✅ STAGE 2 complete — `data_stage_2` ready\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STAGE 2 · RESILIENCE METRICS & BASELINE RATIOS\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "Self-contained, works both as a stand-alone script and inside the same\n",
    "Python session right after Stage 1.\n",
    "\n",
    "Key features\n",
    "• No manual parameters – everything comes from pipeline_config.yaml.\n",
    "• Robust when __file__ is missing (e.g. Jupyter).\n",
    "• Accepts either int or str keys under events: in the YAML.\n",
    "• Re-uses `data_stage_1` if it exists in memory; otherwise loads the\n",
    "  latest Stage 1 CSV.\n",
    "• Produces  \n",
    "  <OUTPUT_ROOT>/event=<SWAN_YEAR>/<RUN_DATE>/stage02/Stage2_Data_WithMetrics.csv\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import os, sys, logging, yaml, io\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 0-3 · UNIVERSAL BOOTSTRAP  (cfg, run-path resolver, params, logger)\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "from pipeline_utils import load_cfg, resolve_run_dir      # NEW shared helper\n",
    "from pathlib import Path\n",
    "import os, sys, logging, io, yaml\n",
    "from typing import Dict, List\n",
    "\n",
    "# 0 · read YAML once ---------------------------------------------------\n",
    "CFG        = load_cfg()\n",
    "defaults: Dict = CFG.get(\"defaults\", {})\n",
    "events:   Dict = CFG.get(\"events\",   {})\n",
    "st2_cfg:  Dict = CFG.get(\"stage2\",   {})\n",
    "\n",
    "# helper: treat YAML keys as both str and int\n",
    "event_keys_str = {str(k): v for k, v in events.items()}\n",
    "\n",
    "# 1 · core params -------------------------------------------------------\n",
    "SWAN_YEAR = str(os.getenv(\"SWAN_YEAR\") or next(iter(event_keys_str)))\n",
    "if SWAN_YEAR not in event_keys_str:\n",
    "    raise KeyError(f\"SWAN_YEAR={SWAN_YEAR} not present in YAML `events:` block\")\n",
    "\n",
    "MAX_YEARS = int(st2_cfg.get(\"max_years\", 4))\n",
    "DATE_COL  = st2_cfg.get(\"date_col\", \"ReportDate\")\n",
    "ID_COL    = st2_cfg.get(\"id_col\",   \"Symbol\")\n",
    "\n",
    "METRICS: List[str] = st2_cfg.get(\n",
    "    \"metrics\",\n",
    "    [\n",
    "        \"NetIncome\", \"EarningBeforeInterestAndTax\", \"OperatingIncome\", \"EBITDA\",\n",
    "        \"OperatingCashFlow\", \"FreeCashFlow\", \"Cash\", \"CashAndCashEquivalents\",\n",
    "        \"TotalRevenue\", \"GrossProfit\",\n",
    "    ],\n",
    ")\n",
    "METRIC_SIGN: Dict[str, bool] = {m: True for m in METRICS}   # True ⇒ higher better\n",
    "\n",
    "# 2 · resolve run folder & paths ---------------------------------------\n",
    "#     • honour $RUN_DIR or $RUN_DATE if the user sets them\n",
    "#     • otherwise pick the latest run that already contains Stage-1 output\n",
    "RUN_DIR   = resolve_run_dir(must_have=\"stage01/stage01_cleaned.csv\")\n",
    "RUN_DATE  = RUN_DIR.name\n",
    "\n",
    "STAGE1_FILE = RUN_DIR / \"stage01\" / \"stage01_cleaned.csv\"\n",
    "OUT_DIR     = RUN_DIR / \"stage02\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3 · logger -----------------------------------------------------------\n",
    "if \"logger\" in globals() and isinstance(globals()[\"logger\"], logging.Logger):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    if not any(isinstance(h, logging.FileHandler) and h.baseFilename.endswith(\"stage02.log\")\n",
    "               for h in logger.handlers):\n",
    "        logger.addHandler(logging.FileHandler(OUT_DIR / \"stage02.log\", mode=\"w\", encoding=\"utf-8\"))\n",
    "else:\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s | %(levelname)-7s | %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(OUT_DIR / \"stage02.log\", mode=\"w\", encoding=\"utf-8\"),\n",
    "            logging.StreamHandler(sys.stdout),\n",
    "        ],\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"==========  STAGE 2: RESILIENCE METRICS ==========\")\n",
    "logger.info(\"RUN_DIR        : %s\", RUN_DIR)\n",
    "logger.info(\"SWAN_YEAR=%s  RUN_DATE=%s  MAX_YEARS=%s\", SWAN_YEAR, RUN_DATE, MAX_YEARS)\n",
    "logger.info(\"DATE_COL / ID_COL = %s / %s\", DATE_COL, ID_COL)\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 4 · LOAD STAGE 1 DATA  (memory → disk fallback)\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "if \"data_stage_1\" in globals():\n",
    "    df = globals()[\"data_stage_1\"].copy()\n",
    "    logger.info(\"Stage 1 data reused from memory.\")\n",
    "else:\n",
    "    if not STAGE1_FILE.is_file():\n",
    "        raise FileNotFoundError(f\"Stage 1 CSV not found at {STAGE1_FILE}\")\n",
    "    df = pd.read_csv(STAGE1_FILE, parse_dates=[DATE_COL], low_memory=False)\n",
    "    logger.info(\"Stage 1 CSV loaded: %s rows\", f\"{len(df):,}\")\n",
    "\n",
    "df[\"Year\"] = df[DATE_COL].dt.year.astype(\"Int16\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 5 · DERIVED COLUMNS\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "if {\"PretaxIncome\", \"EffectiveTaxRateAsReported\"}.issubset(df.columns):\n",
    "    df[\"IncomeTaxExpense\"] = df[\"PretaxIncome\"] * df[\"EffectiveTaxRateAsReported\"]\n",
    "    med = df.groupby([\"SectorName\", \"Year\"])[\"IncomeTaxExpense\"].transform(\"median\")\n",
    "    df[\"IncomeTaxExpense\"].fillna(med, inplace=True)\n",
    "else:\n",
    "    df[\"IncomeTaxExpense\"] = np.nan\n",
    "    logger.warning(\"IncomeTaxExpense derivation skipped (missing inputs)\")\n",
    "\n",
    "if {\"TotalAssets\", \"TotalLiabilitiesAsReported\"}.issubset(df.columns):\n",
    "    df[\"NetAssets\"] = df[\"TotalAssets\"] - df[\"TotalLiabilitiesAsReported\"]\n",
    "elif {\"TotalAssets\", \"TotalLiabilities\"}.issubset(df.columns):\n",
    "    df[\"NetAssets\"] = df[\"TotalAssets\"] - df[\"TotalLiabilities\"]\n",
    "else:\n",
    "    df[\"NetAssets\"] = np.nan\n",
    "    logger.warning(\"NetAssets derivation skipped (missing inputs)\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 6 · HELPERS\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "SWAN_YEAR_INT = int(SWAN_YEAR)           # for numeric comparison\n",
    "\n",
    "def _last_pre_swan(series: pd.Series) -> float:\n",
    "    pre = series.dropna()\n",
    "    pre = pre[pre.index < SWAN_YEAR_INT]\n",
    "    return pre.iloc[-1] if not pre.empty else np.nan\n",
    "\n",
    "def _first_recovery(series: pd.Series, baseline: float, higher_ok: bool) -> float:\n",
    "    if pd.isna(baseline):\n",
    "        return np.nan\n",
    "    cond = series >= baseline if higher_ok else series <= baseline\n",
    "    cand = series[(series.index >= SWAN_YEAR_INT) & cond]\n",
    "    return cand.index.min() if not cand.empty else np.nan\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 7 · METRIC LOOP\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "for metric in METRICS:\n",
    "    if metric not in df.columns:\n",
    "        logger.warning(\"⏭️ %-30s missing — skipped\", metric)\n",
    "        continue\n",
    "\n",
    "    grp = df.set_index(\"Year\").groupby(ID_COL)[metric]\n",
    "\n",
    "    baseline = grp.apply(_last_pre_swan).rename(\"Baseline\")\n",
    "    rec_year = grp.apply(\n",
    "        lambda s: _first_recovery(s, baseline.get(s.name), METRIC_SIGN[metric])\n",
    "    ).rename(\"RecYear\")\n",
    "\n",
    "    rp = (rec_year - SWAN_YEAR_INT + 1).clip(lower=1, upper=MAX_YEARS) \\\n",
    "                                       .fillna(MAX_YEARS).astype(\"int16\")\n",
    "    score = ((rp - 1) / (MAX_YEARS - 1)).round(4)     # 0 best … 1 worst\n",
    "    flag  = (rp < rp.median()).astype(\"int8\")\n",
    "\n",
    "    df = (\n",
    "        df.merge(rp.rename(f\"RP_{metric}\"),       on=ID_COL, how=\"left\")\n",
    "          .merge(score.rename(f\"Score_{metric}\"), on=ID_COL, how=\"left\")\n",
    "          .merge(flag.rename(f\"Flag_{metric}\"),   on=ID_COL, how=\"left\")\n",
    "    )\n",
    "\n",
    "    logger.info(\"%-30s baseline %.1f%% | recovery %.1f%%\",\n",
    "                metric, baseline.notna().mean()*100, rp.notna().mean()*100)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 8 · EXPORT\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "out_csv = OUT_DIR / \"Stage2_Data_WithMetrics.csv\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "\n",
    "buf = io.StringIO(); df.info(buf=buf)\n",
    "logger.info(\"Final DataFrame info:\\n%s\", buf.getvalue())\n",
    "logger.info(\"Saved Stage 2 CSV → %s\", out_csv)\n",
    "\n",
    "data_stage_2 = df.copy()\n",
    "logger.info(\"✅ STAGE 2 complete — `data_stage_2` ready\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21.140291,
   "end_time": "2025-06-11T04:40:20.049878",
   "environment_variables": {},
   "exception": null,
   "input_path": "stage02.ipynb",
   "output_path": "stage02_output.ipynb",
   "parameters": {
    "INPUT_CSV": "C:/Users/Jason Pohl/OneDrive - Bond University/PhD/rff/NEW_DATA.csv",
    "OUTPUT_ROOT": "C:/Users/Jason Pohl/OneDrive - Bond University/PhD/rff/outputs_rff",
    "RUN_TAG": "myUniqueRunId",
    "STAGE1_CFG": "",
    "SWAN_YEAR": 2008,
    "WIN_END": 2012,
    "WIN_START": 2004
   },
   "start_time": "2025-06-11T04:39:58.909587",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}